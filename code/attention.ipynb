{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Comparing traditional word embeddings to attention-based\n",
    "\n",
    "As we have discussed, deep learning can be thought of as a two part process where the first part involves extracting features from the input. In this lab, we'll look at how even when using a simple model in the second half, improving the quality of feature extraction can dramatically improve the performance of the model.\n",
    "\n",
    "We will compare the performance of two distinct approaches to embedding - the classic GLOVE embeddings, and newer, attention-based embeddings. We will use a simple logistic regression model to classify movie reviews as positive or negative, and compare the performance of the two models.\n",
    "\n",
    "# Setup\n",
    "\n",
    "We will be using the `sentence-transformers` library to get transformer embeddings, the `gensim` library to get GLOVE embedding, and the `scikit-learn` library to train our model."
   ],
   "id": "ba7763db6b9ebd42"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "!pip install -U -q sentence-transformers gensim scikit-learn openpyxl",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_excel('https://github.com/laxmimerit/IMDB-Movie-Reviews-Large-Dataset-50k/raw/refs/heads/master/train.xlsx')"
   ],
   "id": "66b29ec7c9b71188",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = df.sample(1000)",
   "id": "fa008396a48c36c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Preprocessing\n",
    "\n",
    "We will preprocess the data by removing stopwords, punctuation, and converting the text to lowercase. Gensim can help us do this easily."
   ],
   "id": "166f887865f7450"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, strip_multiple_whitespaces, strip_numeric\n",
    "\n",
    "# Preprocess the text\n",
    "CUSTOM_FILTERS = [lambda x: x.lower(), strip_multiple_whitespaces, strip_numeric, strip_punctuation, remove_stopwords]\n",
    "df['cleaned_review'] = df['Reviews'].astype(str).apply(lambda x: preprocess_string(x, CUSTOM_FILTERS))\n",
    "df.sample(5)"
   ],
   "id": "199e62cf959ca58e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df['Sentiment'] = df['Sentiment'].map({'pos': 1, 'neg': 0})\n",
    "df['cleaned_review'] = df['cleaned_review'].apply(lambda x: ' '.join(x))"
   ],
   "id": "886ee989fb3de75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.sample(5)",
   "id": "4e37405e26695370",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_review'], df['Sentiment'], test_size=0.2, random_state=42)"
   ],
   "id": "8324ccf4a12793dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# GLOVE Embeddings\n",
    "\n",
    "We will use the `gensim` library to load the GLOVE embeddings. We will use the 100-dimensional embeddings trained on the Wikipedia 2014 + Gigaword 5 dataset, because the file size is fairly small (128MB) and it is a good balance between size and quality. GLOVE is similar to Word2Vec, but uses a different algorithm to train the embeddings."
   ],
   "id": "50a005f44be35e18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from gensim import downloader\n",
    "\n",
    "# Load the GLOVE embeddings\n",
    "glove_vectors = downloader.load('glove-wiki-gigaword-100')"
   ],
   "id": "8e52463609aee978",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "glove_vectors.most_similar('good')",
   "id": "f3564ed708cdecd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Preprocess the text\n",
    "\n",
    "def get_glove_embedding(text):\n",
    "    # Get the embeddings for each word\n",
    "    embeddings = [glove_vectors[word] for word in text if word in glove_vectors]\n",
    "    if len(embeddings) == 0:\n",
    "        return None\n",
    "    # Average the embeddings\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "# Get the embeddings for the training and testing sets\n",
    "X_train_glove = X_train.apply(get_glove_embedding)\n",
    "X_test_glove = X_test.apply(get_glove_embedding)"
   ],
   "id": "b12a44578ef0f351",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Transformer Embeddings\n",
    "\n",
    "We will use the `sentence-transformers` library to get our transformer embeddings. We will use the `paraphrase-MiniLM-L3-v2` model, which is the smallest (and thus fastest) model available in the library."
   ],
   "id": "8fb0860eca05d1fe"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the transformer embeddings\n",
    "print('Loading transformer embeddings...')\n",
    "transformer = SentenceTransformer('paraphrase-MiniLM-L3-v2', model_kwargs={'torch_dtype': 'float16'})\n",
    "\n",
    "# Get the embeddings for the training and testing sets\n",
    "print('Getting embeddings for the training set...')\n",
    "X_train_bert = transformer.encode(X_train.tolist())\n",
    "print('Getting embeddings for the testing set...')\n",
    "X_test_bert = transformer.encode(X_test.tolist())"
   ],
   "id": "5562825af4c22c33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training the Model\n",
    "\n",
    "We will use a simple logistic regression model to classify the reviews as positive or negative. We will train the model using the GLOVE embeddings, and then using the BERT embeddings, and compare the performance of the two models."
   ],
   "id": "402b60aff9418f8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train the model using the GLOVE embeddings\n",
    "glove_lr = LogisticRegression()\n",
    "glove_lr.fit(X_train_glove.tolist(), y_train)\n",
    "\n",
    "# Train the model using the BERT embeddings\n",
    "trfrm_lr = LogisticRegression()\n",
    "trfrm_lr.fit(X_train_bert, y_train)\n",
    "\n",
    "# Evaluate the models\n",
    "glove_score = glove_lr.score(X_test_glove.tolist(), y_test)\n",
    "bert_score = trfrm_lr.score(X_test_bert, y_test)\n",
    "\n",
    "print(\"--- GLOVE Embeddings ---\")\n",
    "print(classification_report(y_test, glove_lr.predict(X_test_glove.tolist())))\n",
    "print(\"--- Transformer Embeddings  ---\")\n",
    "print(classification_report(y_test, trfrm_lr.predict(X_test_bert)))"
   ],
   "id": "fd567f05c04601e6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
