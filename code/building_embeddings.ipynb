{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lab 1: Building Embeddings\n",
    "\n",
    "In this lab, we will look at the process of constructing word embeddings from an unlabelled corpus of texts. We'll use the `NLTK` (Natural Language Toolkit) library to preprocess the text data and build embeddings using the `Word2Vec` approach.\n",
    "\n",
    "Before we begin, we need to install the `nltk` library. You can do this by running the following command:"
   ],
   "id": "e369021f5f928e18"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "!pip install -q -U nltk seaborn",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 1: Loading and pre-processing the data\n",
    "\n",
    "Building word embeddings begins with a (ideally large) corpus of real text data. The aim for this dataset is to be representative of the type of text data that the embeddings will be used on. If you're building embeddings for a specific domain, it's a good idea to use text data from that domain so that there are plenty of examples for niche or domain-specific words.\n",
    "\n",
    "We will use the Reuters corpus from `nltk`, which is a collection of news articles. Let's go ahead and load the data to see some examples:"
   ],
   "id": "c92e6f28d4b177ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ],
   "id": "e7d9d968d14a90d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from nltk.corpus import reuters\n",
    "\n",
    "# Display the first few documents in the corpus\n",
    "for file_id in reuters.fileids()[:5]:\n",
    "    print(file_id, reuters.raw(file_id)[:200], '...')"
   ],
   "id": "42f7279e6f7bbd2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display the number of documents in the corpus\n",
    "len(reuters.fileids())"
   ],
   "id": "10030656eb986080",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The key principle that word embeddings are built on is that the meaning of a word can be inferred from the context in which it appears. This is the basis of the `Word2Vec` algorithm, which learns to predict a word given its context (or vice versa) by training a neural network on a large corpus of text.\n",
    "\n",
    "To prepare the data for training the `Word2Vec` model, we need to perform the following steps:\n",
    "\n",
    "1. Tokenize the text: Split the text into individual words (or tokens).\n",
    "2. Remove punctuation and special characters.\n",
    "3. Convert all words to lowercase.\n",
    "\n",
    "This way, we can ensure that the model learns to treat words like \"hello\", \"Hello\", and \"HELLO\" as the same word.\n",
    "\n",
    "`nltk` provides some useful tools for text processing, which we'll use for this lab. In practice, `nltk` isn't the most efficient library for processing large datasets, but it's great for educational purposes. If you're curious about more efficient text processing libraries, you can look into `spaCy` and `gensim`."
   ],
   "id": "5792f40975ab84b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 1: Tokenize the text\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "print('Before:\\n', reuters.raw('test/14826')[:200])\n",
    "print('After:\\n', tokenize(reuters.raw('test/14826'))[:20])"
   ],
   "id": "a3c0038f55c54f49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next we will remove punctuation and special characters from the tokens. There are different ways we can approach this depending on the requirements of the task. For example, our tokenizer (which is very basic) has created 'U.S.-JAPAN' as a single token, because there's no space. If we remove punctuation, we'll end up with 'USJAPAN', which might not be what we want.\n",
    "\n",
    "For this exercise, we will simply remove any token that contains a non-alphanumeric character. This will remove tokens like 'U.S.', but it will also remove tokens like 'it's' and 'can't'. This is far from best practice, but it's simple and will work for our purposes."
   ],
   "id": "4b155875becc1542"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 2: Remove punctuation and special characters\n",
    "\n",
    "def remove_punctuation(tokens):\n",
    "    return [word for word in tokens if word.isalnum()]\n",
    "\n",
    "tokens = tokenize(reuters.raw('test/14826'))\n",
    "print('Before:\\n', tokens[:20])\n",
    "print('After:\\n', remove_punctuation(tokens)[:20])"
   ],
   "id": "5de67e7f5c8094c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, we will convert all tokens to lowercase.",
   "id": "b61b88277ee1766a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 3: Convert all words to lowercase\n",
    "\n",
    "def to_lowercase(tokens):\n",
    "    return [word.lower() for word in tokens]\n",
    "\n",
    "tokens = remove_punctuation(tokenize(reuters.raw('test/14826')))\n",
    "print('Before:\\n', tokens[:20])\n",
    "print('After:\\n', to_lowercase(tokens)[:20])"
   ],
   "id": "356491b43f03e061",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise: Combining the preprocessing steps\n",
    "\n",
    "Now that we have defined the three preprocessing steps, let's combine them into a single function that takes a text string as input and returns a list of preprocessed tokens.\n",
    "\n",
    "Complete the function `preprocess_text` below to combine the three steps:\n",
    "\n",
    "1. Tokenize the text.\n",
    "2. Remove punctuation and special characters.\n",
    "3. Convert all words to lowercase.\n",
    "\n",
    "The function should return a list of preprocessed tokens, where each token is a string."
   ],
   "id": "6b915bfe77300e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def preprocess_text(text):\n",
    "    # Complete the function\n",
    "\n",
    "# Test the function on a sample text\n",
    "sample_text = reuters.raw('test/14826')\n",
    "preprocess_text(sample_text)[:20]"
   ],
   "id": "a9ccb2051039ebe3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "There are many other preprocessing steps that can be applied to text data, such as:\n",
    "\n",
    "- Removing stopwords (common words like \"the\", \"a\", \"is\", etc.).\n",
    "- Stemming or lemmatization to reduce words to their base form (e.g. \"running\" -> \"run\", \"raised\" -> \"raise\").\n",
    "- Removing rare words or words that appear too frequently.\n",
    "\n",
    "## Building a vocabulary\n",
    "\n",
    "Now that we have a function to preprocess text data, we can use it to build a vocabulary of words from the Reuters corpus. The vocabulary is simply a set of unique words that appear in the corpus. Typically, it's not worth it to include very rare words in the vocabulary, as we won't have enough examples to learn good embeddings for them. In our case, we also just want to speed things up down the line, so we will limit ourselves to the 1000 most common words in the corpus (this is a really small vocabulary!)"
   ],
   "id": "8a689369e88a53b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "VOCAB_SIZE = 1000\n",
    "\n",
    "# Build the vocabulary\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def build_vocabulary(corpus):\n",
    "    word_counts = Counter()\n",
    "    for file_id in tqdm(corpus.fileids(), total=len(corpus.fileids())):\n",
    "        text = corpus.raw(file_id)\n",
    "        tokens = preprocess_text(text)\n",
    "        word_counts.update(tokens)\n",
    "    return [word for word, _ in word_counts.most_common(VOCAB_SIZE)]\n",
    "\n",
    "vocabulary = build_vocabulary(reuters)\n",
    "\n",
    "print(f\"First ten words in the vocabulary: {vocabulary[:10]}\")\n",
    "print(f\"Last ten words in the vocabulary: {vocabulary[-10:]}\")"
   ],
   "id": "4eab6b975dc5c273",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 2: Building a Co-occurrence Matrix\n",
    "\n",
    "The next step in building word embeddings is to construct a co-occurrence matrix. This matrix will have a row and column for each word in the vocabulary, and each cell will contain the number of times the corresponding words appear together in a window of a fixed size.\n",
    "\n",
    "The window size is a hyperparameter that determines how many words before and after the target word are considered to be its context. A larger window size captures more semantic information, but reduces the specificity of the embeddings. A window size of 2-10 is common for small to medium-sized datasets.\n",
    "\n",
    "To build the co-occurrence matrix, we need to perform the following steps:\n",
    "\n",
    "1. Create a mapping from words to indices in the vocabulary: each word will have a unique index that corresponds to its row and column in the matrix.\n",
    "2. Iterate over the corpus and count co-occurrences of words within the window.\n",
    "3. Construct a sparse matrix from the co-occurrence counts.\n",
    "4. Normalize the matrix to account for the frequency of individual words.\n",
    "\n",
    "Let's begin by creating our mapping from words to indices. This is fairly straightforward, as we can use the index of the word in the vocabulary list as its index in the matrix. So for example, the word at index 0 in the vocabulary (\"the\") will have index 0 in the matrix, and so on."
   ],
   "id": "1a3b5ba24177e27c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 1: Create a mapping from words to indices in the vocabulary\n",
    "\n",
    "word_to_index = {word: i for i, word in enumerate(vocabulary)}"
   ],
   "id": "5685a3e9cb0ea607",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, we will iterate over the corpus and count co-occurrences of words within a window of size `window_size`. To do this, we will use a `defaultdict` of `Counter` objects, which allows us to efficiently count the co-occurrences of words in the context of each target word.\n",
    "\n",
    "For each word in the corpus, we will consider the words within a window of size `window_size` before and after the target word. We will increment the co-occurrence counts for the target word and the context words in the `co_occurrence_counts` dictionary."
   ],
   "id": "3d5c1dc0d189f838"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_co_occurrence_matrix(texts, vocabulary, word_to_index, window_size=2):\n",
    "    co_occurrence_counts = defaultdict(Counter)\n",
    "    for text in tqdm(texts, total=len(texts)):\n",
    "        for i, target_word in enumerate(text):\n",
    "            if target_word in vocabulary:\n",
    "                target_index = word_to_index[target_word]\n",
    "                context = text[max(i - window_size, 0):i] + text[i + 1:i + window_size + 1]\n",
    "                for context_word in context:\n",
    "                    if context_word in word_to_index:\n",
    "                        context_index = word_to_index[context_word]\n",
    "                        co_occurrence_counts[target_index][context_index] += 1\n",
    "                        \n",
    "    return co_occurrence_counts\n",
    "\n",
    "# To ease our understanding, let's begin by building a co-occurrence matrix for just one document\n",
    "\n",
    "sample_text = preprocess_text(reuters.raw('test/14826'))\n",
    "\n",
    "co_occurrence_counts = build_co_occurrence_matrix([sample_text], vocabulary, word_to_index, window_size=2)\n",
    "\n",
    "# Display the co-occurrence counts for the word \"the\"\n",
    "for index, count in co_occurrence_counts[word_to_index['the']].items():\n",
    "    print(vocabulary[index], count)"
   ],
   "id": "d495090f02d064d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we have the co-occurrence counts for a single document, we can extend this to the entire corpus. We will build the co-occurrence matrix for the entire Reuters corpus, using a window size of 2. This will take a moment to run.",
   "id": "8ac0f1f0c1785ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "co_occurrence_counts = build_co_occurrence_matrix([preprocess_text(reuters.raw(file_id)) for file_id in reuters.fileids()], vocabulary, word_to_index, window_size=2)",
   "id": "f2954e1f2589430f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display the co-occurrence counts for the word \"the\"\n",
    "for index, count in co_occurrence_counts[word_to_index['the']].items():\n",
    "    print(vocabulary[index], count)"
   ],
   "id": "8d83572e2375957d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 3: Constructing a Sparse Matrix\n",
    "\n",
    "The co-occurrence matrix is typically very sparse, as most words will not appear together in the corpus. To save memory and computation time, we can store the matrix in a sparse format. We will use the `csr_matrix` class from the `scipy` library, which is an efficient way to store sparse matrices.\n",
    "\n",
    "Note that this is a very small example, so the matrix will not be very sparse. For larger datasets, the sparsity will be much higher, and so the savings from this step will be more significant."
   ],
   "id": "765680a27f4aa884"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def build_sparse_matrix(co_occurrence_counts, vocabulary_size):\n",
    "    rows, cols, data = [], [], []\n",
    "    for i, counter in co_occurrence_counts.items():\n",
    "        for j, count in counter.items():\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            data.append(count)\n",
    "    return csr_matrix((data, (rows, cols)), shape=(vocabulary_size, vocabulary_size))\n",
    "\n",
    "co_occurrence_matrix = build_sparse_matrix(co_occurrence_counts, VOCAB_SIZE)"
   ],
   "id": "5f946db4c2e5faae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display the shape of the co-occurrence matrix\n",
    "co_occurrence_matrix.shape"
   ],
   "id": "8b00c60dc8036d6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 4: Normalizing the Matrix\n",
    "\n",
    "The final step in building the co-occurrence matrix is to normalize it to account for the frequency of individual words. This is done by dividing each cell in the matrix by the sum of the row (or column) in which it appears. This ensures that words that appear more frequently in the corpus do not dominate the embeddings."
   ],
   "id": "afcb28f3812d1e67"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def normalize_matrix(matrix):\n",
    "    row_sums = matrix.sum(axis=1)\n",
    "    row_sums[row_sums == 0] = 1\n",
    "    return matrix / row_sums\n",
    "\n",
    "co_occurrence_matrix = normalize_matrix(co_occurrence_matrix)"
   ],
   "id": "887ec5b8c028c113",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we have a normalized co-occurrence matrix where the value in cell (i,j) represents something like the likelihood of seeing words i and j near each other. What's interesting to note is that this is already sufficient to get some useful information about the relationships between words.\n",
    "\n",
    "Remember, the key principle behind word embeddings is that similar words should have similar neighbours. With that in mind, we can estimate the similarity of two words by comparing their co-occurrence vectors. We can use the cosine similarity metric for this, which measures the cosine of the angle between two vectors. If the vectors are similar, the cosine similarity will be close to 1; if they are dissimilar, it will be close to -1."
   ],
   "id": "e7ee4421683adcfc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return vec1.dot(vec2.T).toarray()[0, 0]\n",
    "\n",
    "def estimate_similarity(word1, word2, matrix, word_to_index):\n",
    "    index1, index2 = word_to_index[word1], word_to_index[word2]\n",
    "    vec1, vec2 = matrix.getrow(index1), matrix.getrow(index2)\n",
    "    return cosine_similarity(vec1, vec2)\n",
    "\n",
    "estimate_similarity('economy', 'economic', co_occurrence_matrix, word_to_index)"
   ],
   "id": "9ee68a0b3b49a0f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "estimate_similarity('economy', 'canada', co_occurrence_matrix, word_to_index)",
   "id": "d7112b7faf3be9da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "...okay, maybe these aren't the most amazing results, but we'll get there! The embeddings we've built so far are very basic, and there are many ways to improve them. Some common techniques include:\n",
    "\n",
    "- Using larger vocabularies and more data.\n",
    "- Tuning hyperparameters like the window size and the number of dimensions in the embeddings.\n",
    "- Applying more advanced text processing techniques.\n",
    "- Using more sophisticated algorithms like GloVe or FastText.\n",
    "\n",
    "The last step we will apply to our own word embeddings is dimensionality reduction. Since our current matrix is of size (VOCAB_SIZE, VOCAB_SIZE), you can imagine that this quickly becomes intractable for large vocabularies. We can use techniques like PCA or t-SNE to reduce the dimensionality of the embeddings while preserving the relationships between words."
   ],
   "id": "7fabcce493cbf4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def reduce_dimensions(matrix, n_components=2):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    return pca.fit_transform(matrix.toarray())\n",
    "\n",
    "embeddings_2d = reduce_dimensions(co_occurrence_matrix, n_components=2)"
   ],
   "id": "cc18797a37b80da8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Congratulations! We now have a set of two values that abstractly represent the meaning of each word in the vocabulary. We can plot these values on a 2D graph to visualize the relationships between words. Let's do that now.",
   "id": "5bf462744dc73c41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_embeddings(embeddings, vocabulary):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, word in enumerate(vocabulary):\n",
    "        x, y = embeddings[i]\n",
    "        plt.plot(x, y, 'bo')\n",
    "        plt.text(x, y, word, fontsize=6)\n",
    "    plt.show()\n",
    "    \n",
    "plot_embeddings(embeddings_2d, vocabulary)"
   ],
   "id": "813bd9acc3b34ef1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Extension: Building Word Embeddings with Word2Vec in Gensim\n",
    "\n",
    "If you have time, you can try building word embeddings using the `Word2Vec` algorithm from the `gensim` library. This is a more advanced and efficient approach to building embeddings, and it can capture more complex relationships between words. The Word2Vec process looks something like this:\n",
    "\n",
    "1. Tokenize and preprocess the text data.\n",
    "2. Create a training corpus of (context, target) word pairs, so that the model learns to predict a word given its context.\n",
    "3. Build a neural network that learns to predict the target word from the context word.\n",
    "4. Train the neural network on the training corpus.\n",
    "5. Extract the weights of the hidden layer as the word embeddings.\n",
    "\n",
    "Here's a simple example of how to build word embeddings using Word2Vec in `gensim`:"
   ],
   "id": "e9ca9653a42277b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install -q -U gensim",
   "id": "be885d178f6c6d4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Tokenize and preprocess the text data\n",
    "texts = [preprocess_text(reuters.raw(file_id)) for file_id in reuters.fileids()]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(texts, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Get the word embeddings\n",
    "word_vectors = model.wv\n",
    "\n",
    "# Display the most similar words to \"economy\"\n",
    "print(f'Most similar to \"economy\": {word_vectors.most_similar(\"economy\")}')\n",
    "\n",
    "# Display the similarity between \"economy\" and \"economic\"\n",
    "print(f'Similarity between \"economy\" and \"economic\": {word_vectors.similarity(\"economy\", \"economic\")}')\n",
    "\n",
    "# Save the word vectors to a file\n",
    "word_vectors.save('word_vectors.kv')"
   ],
   "id": "77507999df15ed3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3ec5a3e53c756b03",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
